{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1yf5nwATfz-"
      },
      "source": [
        "# Audio-Based Material Classification\n",
        "\n",
        "Alexis Powell, Zitong Ren, and Jiaming Li\n",
        "\n",
        "CIS 5190/4190\n",
        "\n",
        "Fall 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCG0hKQzLzlN",
        "outputId": "7e241cfb-7bba-40dc-b6d9-facfea4cabdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg\n",
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import librosa.display\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, models, datasets\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U4HNKBEWaob",
        "outputId": "e70c4d8b-f17b-47e6-f8f9-b615f101d5ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgjsS0UoPMUt",
        "outputId": "65f7c678-5a7e-4e33-879c-55b88677e389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ben  blackboard  glass\trailing  sofa  table  water\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/Shareddrives/ML Project/Data/Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FetjhpikeurY"
      },
      "outputs": [],
      "source": [
        "# Define the class-to-label mappings\n",
        "CLASS_TO_LABEL = {\n",
        "    'water': 0,\n",
        "    'table': 1,\n",
        "    'sofa': 2,\n",
        "    'railing': 3,\n",
        "    'glass': 4,\n",
        "    'blackboard': 5,\n",
        "    'ben': 6\n",
        "}\n",
        "\n",
        "# Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 35\n",
        "num_classes = len(CLASS_TO_LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz9Uy6r_aUE8"
      },
      "outputs": [],
      "source": [
        "def calculate_label_statistics(dataset):\n",
        "  labels = [label for _, label in dataset]\n",
        "  mean = np.mean(labels)\n",
        "  std = np.std(labels)\n",
        "  return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpxuGAlM4Y_Q"
      },
      "outputs": [],
      "source": [
        "class MelSpectrogramDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None, mean=None, std=None):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        # Traverse data directory and load images and labels\n",
        "        for class_name, label in CLASS_TO_LABEL.items():\n",
        "            class_dir = os.path.join(data_dir, class_name)\n",
        "            if os.path.exists(class_dir):\n",
        "                for file_name in os.listdir(class_dir):\n",
        "                    if file_name.endswith('.png'):\n",
        "                        self.data.append(os.path.join(class_dir, file_name))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Standardize label using the mean and std\n",
        "        # if self.mean is not None and self.std is not None:\n",
        "            # label = (label - self.mean) / self.std\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def denormalize_label(predicted_label, mean, std):\n",
        "  return predicted_label * std + mean\n",
        "\n",
        "# Define transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load dataset and split into train/test sets\n",
        "# data_path = '/content/drive/Shareddrives/ML Project/mel_spectrograms'\n",
        "data_path = '//content/drive/Shareddrives/ML Project/mel_spectrograms'\n",
        "\n",
        "# dataset_initial = MelSpectrogramDataset(data_path, transform=transform)\n",
        "# mean, std = calculate_label_statistics(dataset_initial)\n",
        "dataset = MelSpectrogramDataset(data_path, transform=transform)\n",
        "\n",
        "train_indices, test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFY_iRGXP7Tc"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "\n",
        "        # Two convolutional layers with kernel size 5 and stride 1\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)  # Padding = 2 to maintain dimensions\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)  # Padding = 2 to maintain dimensions\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Reduces spatial dimensions by half\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling for fc1\n",
        "        self.flattened_size = 64 * 32 * 32  # Adjust based on input size (assumed 128x128 input image)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(self.flattened_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layers with ReLU and pooling\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten feature maps dynamically\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        x = self.fc1(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STWCsKjUQBPb",
        "outputId": "d3d6388e-607f-471d-8cc5-e52c0672fbe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5\n",
            "Fold 1 Results:\n",
            " Train: Loss = 0.0755, Accuracy = 0.9724\n",
            " Validation: Loss = 0.1987, Accuracy = 0.9562\n",
            "Fold 2/5\n",
            "Fold 2 Results:\n",
            " Train: Loss = 0.1162, Accuracy = 0.9595\n",
            " Validation: Loss = 0.0773, Accuracy = 0.9736\n",
            "Fold 3/5\n",
            "Fold 3 Results:\n",
            " Train: Loss = 0.0960, Accuracy = 0.9674\n",
            " Validation: Loss = 0.0608, Accuracy = 0.9788\n",
            "Fold 4/5\n",
            "Fold 4 Results:\n",
            " Train: Loss = 0.1076, Accuracy = 0.9624\n",
            " Validation: Loss = 0.0714, Accuracy = 0.9761\n",
            "Fold 5/5\n",
            "Fold 5 Results:\n",
            " Train: Loss = 0.0956, Accuracy = 0.9662\n",
            " Validation: Loss = 0.0538, Accuracy = 0.9834\n"
          ]
        }
      ],
      "source": [
        "# K-Fold Cross-Validation\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "# Define batch size and learning rate\n",
        "batch_size = 16\n",
        "learning_rate = 0.0005\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(dataset)):\n",
        "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
        "\n",
        "    # Subsets for the current fold\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    test_dataset = Subset(dataset, test_indices)\n",
        "    val_subset = Subset(dataset, val_indices)\n",
        "\n",
        "    # Create DataLoaders for the current fold\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Instantiate a fresh model for each fold\n",
        "    model = CNNClassifier(num_classes=7).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the model with per-epoch performance logging\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
        "        all_labels, all_predictions = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "                total_val += labels.size(0)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(test_loader.dataset)\n",
        "        val_accuracy = correct_val / total_val\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "    fold_results.append({\n",
        "        \"fold\": fold + 1,\n",
        "        \"train_loss\": np.mean(train_losses),\n",
        "        \"val_loss\": np.mean(val_losses),\n",
        "        \"train_accuracy\": np.mean(train_accuracies),\n",
        "        \"val_accuracy\": np.mean(val_accuracies),\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {fold + 1} Results:\")\n",
        "    print(f\" Train: Loss = {np.mean(train_losses):.4f}, Accuracy = {np.mean(train_accuracies):.4f}\")\n",
        "    print(f\" Validation: Loss = {np.mean(val_losses):.4f}, Accuracy = {np.mean(val_accuracies):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold Cross-Validation\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "# Define batch size and learning rate\n",
        "batch_size = 16\n",
        "learning_rate = 0.0005\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(dataset)):\n",
        "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
        "\n",
        "    # Subsets for the current fold\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    test_dataset = Subset(dataset, test_indices)\n",
        "    val_subset = Subset(dataset, val_indices)\n",
        "\n",
        "    # Create DataLoaders for the current fold\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Instantiate a fresh model for each fold\n",
        "    model = CNNClassifier(num_classes=7).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the model with per-epoch performance logging\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
        "        all_labels, all_predictions = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "                total_val += labels.size(0)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(test_loader.dataset)\n",
        "        val_accuracy = correct_val / total_val\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "    fold_results.append({\n",
        "        \"fold\": fold + 1,\n",
        "        \"train_loss\": np.mean(train_losses),\n",
        "        \"val_loss\": np.mean(val_losses),\n",
        "        \"train_accuracy\": np.mean(train_accuracies),\n",
        "        \"val_accuracy\": np.mean(val_accuracies),\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {fold + 1} Results:\")\n",
        "    print(f\" Train: Loss = {np.mean(train_losses):.4f}, Accuracy = {np.mean(train_accuracies):.4f}\")\n",
        "    print(f\" Validation: Loss = {np.mean(val_losses):.4f}, Accuracy = {np.mean(val_accuracies):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vov9key8cI5",
        "outputId": "48cf34ff-3bbf-4da8-9c76-7fa08e15bd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5\n",
            "Fold 1 Results:\n",
            " Train: Loss = 0.1169, Accuracy = 0.9578\n",
            " Validation: Loss = 0.2334, Accuracy = 0.9476\n",
            "Fold 2/5\n",
            "Fold 2 Results:\n",
            " Train: Loss = 0.0860, Accuracy = 0.9692\n",
            " Validation: Loss = 0.0608, Accuracy = 0.9791\n",
            "Fold 3/5\n",
            "Fold 3 Results:\n",
            " Train: Loss = 0.1225, Accuracy = 0.9571\n",
            " Validation: Loss = 0.0851, Accuracy = 0.9709\n",
            "Fold 4/5\n",
            "Fold 4 Results:\n",
            " Train: Loss = 0.0875, Accuracy = 0.9703\n",
            " Validation: Loss = 0.0513, Accuracy = 0.9836\n",
            "Fold 5/5\n",
            "Fold 5 Results:\n",
            " Train: Loss = 0.0944, Accuracy = 0.9671\n",
            " Validation: Loss = 0.0531, Accuracy = 0.9820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4th5AXqFXZcS",
        "outputId": "6169a98e-d7b4-4ee3-96c4-5052233dfe2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final K-Fold Results (Averaged):\n",
            " Train Loss: 0.1014\n",
            " Validation Loss: 0.0967\n",
            " Train Accuracy: 0.9643\n",
            " Validation Accuracy: 0.9726\n"
          ]
        }
      ],
      "source": [
        "# Calculate average metrics across all folds\n",
        "avg_results = {\n",
        "    \"train_loss\": np.mean([result[\"train_loss\"] for result in fold_results]),\n",
        "    \"val_loss\": np.mean([result[\"val_loss\"] for result in fold_results]),\n",
        "    \"train_accuracy\": np.mean([result[\"train_accuracy\"] for result in fold_results]),\n",
        "    \"val_accuracy\": np.mean([result[\"val_accuracy\"] for result in fold_results]),\n",
        "}\n",
        "\n",
        "print(\"\\nFinal K-Fold Results (Averaged):\")\n",
        "print(f\" Train Loss: {avg_results['train_loss']:.4f}\")\n",
        "print(f\" Validation Loss: {avg_results['val_loss']:.4f}\")\n",
        "print(f\" Train Accuracy: {avg_results['train_accuracy']:.4f}\")\n",
        "print(f\" Validation Accuracy: {avg_results['val_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stZfAH59czfi"
      },
      "source": [
        "# Exploratory Experiments"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}